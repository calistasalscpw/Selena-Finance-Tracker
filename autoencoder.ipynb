{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3l1WBviGjzP5",
        "outputId": "cfd7edc5-26b3-4528-bdfe-1a256c659d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.1146\n",
            "Epoch 2/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0605\n",
            "Epoch 3/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0351\n",
            "Epoch 4/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0183\n",
            "Epoch 5/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0154\n",
            "Epoch 6/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0047\n",
            "Epoch 7/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0036\n",
            "Epoch 8/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027\n",
            "Epoch 9/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026\n",
            "Epoch 10/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0030\n",
            "Epoch 11/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0039\n",
            "Epoch 12/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0026\n",
            "Epoch 13/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0023\n",
            "Epoch 14/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0027\n",
            "Epoch 15/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
            "Epoch 16/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 17/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0026\n",
            "Epoch 18/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0028\n",
            "Epoch 19/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037\n",
            "Epoch 20/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0026\n",
            "Epoch 21/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0019\n",
            "Epoch 22/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021\n",
            "Epoch 23/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0023\n",
            "Epoch 24/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0018\n",
            "Epoch 25/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
            "Epoch 26/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 27/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 29/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 30/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
            "Epoch 31/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 32/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 33/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020\n",
            "Epoch 34/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 35/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0024\n",
            "Epoch 36/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
            "Epoch 37/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
            "Epoch 38/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 39/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0019\n",
            "Epoch 40/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
            "Epoch 41/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 42/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0022\n",
            "Epoch 43/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
            "Epoch 44/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0023\n",
            "Epoch 45/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0018\n",
            "Epoch 46/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0017\n",
            "Epoch 47/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0021\n",
            "Epoch 48/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0020\n",
            "Epoch 49/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0015\n",
            "Epoch 50/50\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0014\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-25e1b6eabda1>:165: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  anomalies['description'] = \"Unusual spending detected\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['date'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-25e1b6eabda1>\u001b[0m in \u001b[0;36m<cell line: 291>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m     \u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomalies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercentile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;31m# Display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-25e1b6eabda1>\u001b[0m in \u001b[0;36mmain_pipeline\u001b[0;34m(data, epochs, batch_size, percentile)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# Step 6: Generate JSON report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mjson_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomalies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_income\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_expense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madvice_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result_data.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_report\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-25e1b6eabda1>\u001b[0m in \u001b[0;36mgenerate_json\u001b[0;34m(processed_data, anomalies, total_income, total_expense, advice_list)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mnet_cashflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_income\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtotal_expense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0manomaly_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manomalies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'description'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'records'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     report = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['date'] not in index\""
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, losses\n",
        "\n",
        "# Function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the input data by extracting date-based features and normalizing values using MinMaxScaler.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): Input dataframe with 'date' and 'amount' columns.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Scaled feature matrix.\n",
        "        pd.DataFrame: Original data with extracted features.\n",
        "        MinMaxScaler: Fitted scaler for potential reuse.\n",
        "    \"\"\"\n",
        "    # Convert 'date' to datetime and extract features\n",
        "    data['date'] = pd.to_datetime(data['date'])\n",
        "    data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
        "    data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
        "    data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
        "    data['year'] = data['date'].dt.year             # Year\n",
        "    data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
        "\n",
        "    # Select relevant features\n",
        "    features = ['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']\n",
        "    data = data[features]\n",
        "\n",
        "    # Normalize the data using MinMaxScaler\n",
        "    scaler = MinMaxScaler()\n",
        "    data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "    return data_scaled, data, scaler\n",
        "\n",
        "\n",
        "# Function to build the autoencoder model\n",
        "def build_autoencoder(input_dim):\n",
        "    \"\"\"\n",
        "    Builds and compiles an autoencoder model.\n",
        "\n",
        "    Parameters:\n",
        "        input_dim (int): Number of input features.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled autoencoder model.\n",
        "    \"\"\"\n",
        "    autoencoder = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),       # Input layer\n",
        "        layers.Dense(32, activation='relu'),    # Encoder\n",
        "        layers.Dense(16, activation='relu'),    # Encoder\n",
        "        layers.Dense(8, activation='relu'),     # Bottleneck\n",
        "        layers.Dense(16, activation='relu'),    # Decoder\n",
        "        layers.Dense(32, activation='relu'),    # Decoder\n",
        "        layers.Dense(input_dim, activation='sigmoid')  # Output layer\n",
        "    ])\n",
        "    autoencoder.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())\n",
        "    return autoencoder\n",
        "\n",
        "# Function to detect anomalies\n",
        "def detect_anomalies(autoencoder, data_scaled, original_data, scaler, percentile=99):\n",
        "    \"\"\"\n",
        "    Detects anomalies using the trained autoencoder.\n",
        "\n",
        "    Parameters:\n",
        "        autoencoder (tf.keras.Model): Trained autoencoder model.\n",
        "        data_scaled (np.ndarray): Normalized input data.\n",
        "        original_data (pd.DataFrame): Original data with features.\n",
        "        scaler (MinMaxScaler): Scaler used for normalization.\n",
        "        percentile (float): Percentile threshold for anomaly detection.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Data with anomaly status and reconstruction errors.\n",
        "        pd.DataFrame: Detected anomalies.\n",
        "    \"\"\"\n",
        "    # Reconstruct the input data\n",
        "    reconstructed = autoencoder.predict(data_scaled)\n",
        "    mse = np.mean(np.power(data_scaled - reconstructed, 2), axis=1)  # Reconstruction error\n",
        "\n",
        "    # Set anomaly threshold\n",
        "    threshold = np.percentile(mse, percentile)\n",
        "\n",
        "    # Flag anomalies\n",
        "    original_data['reconstruction_error'] = mse\n",
        "    original_data['is_anomaly'] = mse > threshold\n",
        "\n",
        "    # Reconstruct 'date' column\n",
        "    original_data['date'] = pd.to_datetime(\n",
        "        original_data['year'] * 1000 + original_data['day_of_year'],\n",
        "        format='%Y%j'\n",
        "    )\n",
        "\n",
        "    # Filter anomalies\n",
        "    anomalies = original_data[original_data['is_anomaly']]\n",
        "\n",
        "    return original_data, anomalies\n",
        "\n",
        "# Function to plot the data\n",
        "def plot_results(processed_data, anomalies):\n",
        "    \"\"\"\n",
        "    Plots the data, highlighting anomalies in red and normal data in blue, with 'date' on the x-axis and 'amount' on the y-axis.\n",
        "\n",
        "    Parameters:\n",
        "        processed_data (pd.DataFrame): Dataframe with anomaly status.\n",
        "        anomalies (pd.DataFrame): Detected anomalies.\n",
        "    \"\"\"\n",
        "    # Reconstruct a synthetic 'date' from year and day_of_year\n",
        "    processed_data['date'] = pd.to_datetime(processed_data['year'] * 1000 + processed_data['day_of_year'], format='%Y%j')\n",
        "    anomalies['date'] = pd.to_datetime(anomalies['year'] * 1000 + anomalies['day_of_year'], format='%Y%j')\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot normal data\n",
        "    normal_data = processed_data[~processed_data['is_anomaly']]\n",
        "    plt.scatter(normal_data['date'], normal_data['amount'], color='blue', label='Normal', alpha=0.6)\n",
        "\n",
        "    # Plot anomalies\n",
        "    plt.scatter(anomalies['date'], anomalies['amount'], color='red', label='Anomaly', alpha=0.8)\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.title('Anomaly Detection in Financial Transactions')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Amount')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Main function to run the pipeline\n",
        "def main_pipeline(data, epochs=50, batch_size=16, percentile=99):\n",
        "    \"\"\"\n",
        "    Main pipeline for anomaly detection using autoencoder.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): Input dataframe with 'date' and 'amount' columns.\n",
        "        epochs (int): Number of training epochs.\n",
        "        batch_size (int): Batch size for training.\n",
        "        percentile (float): Percentile threshold for anomaly detection.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Trained autoencoder model.\n",
        "        pd.DataFrame: Processed data with anomaly status.\n",
        "        pd.DataFrame: Detected anomalies.\n",
        "        MinMaxScaler: Scaler used for data normalization.\n",
        "        float: Threshold for anomaly detection based on reconstruction error.\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess the data\n",
        "    data_scaled, processed_data, scaler = preprocess_data(data)\n",
        "\n",
        "    # Step 2: Build the autoencoder\n",
        "    input_dim = data_scaled.shape[1]\n",
        "    autoencoder = build_autoencoder(input_dim)\n",
        "\n",
        "    # Step 3: Train the autoencoder\n",
        "    history = autoencoder.fit(data_scaled, data_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
        "\n",
        "    # Step 4: Detect anomalies\n",
        "    processed_data, anomalies = detect_anomalies(autoencoder, data_scaled, processed_data, scaler, percentile=percentile)\n",
        "\n",
        "    # Step 5: Financial metrics for cashflow analysis\n",
        "    total_income = data[data['amount'] > 0]['amount'].sum()\n",
        "    total_expense = data[data['amount'] > 0]['amount'].sum()\n",
        "    advice_list = [\n",
        "        \"Reduce unnecessary spending\",\n",
        "        \"Plan your spendings wisely\"\n",
        "    ]\n",
        "    anomalies['description'] = \"Unusual spending detected\"\n",
        "\n",
        "    # Step 6: Generate JSON report\n",
        "    json_report = generate_json(processed_data, anomalies, total_income, total_expense, advice_list)\n",
        "    with open(\"result_data.json\", \"w\") as json_file:\n",
        "        json_file.write(json_report)\n",
        "\n",
        "    # Step 7: Plot results\n",
        "    plot_results(processed_data, anomalies)\n",
        "\n",
        "    return autoencoder, processed_data, anomalies, scaler\n",
        "\n",
        "def predict_pipeline(model, data, scaler, percentile):\n",
        "    \"\"\"\n",
        "    Pipeline to preprocess data, predict anomalies, and plot results using a trained model.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): Trained autoencoder model.\n",
        "        data (pd.DataFrame): New data with the same structure as training data.\n",
        "        scaler (MinMaxScaler): Scaler used to preprocess the training data.\n",
        "        percentile (float): Percentile for anomaly detection based on reconstruction error.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: New data with anomaly status and reconstruction errors.\n",
        "        pd.DataFrame: Detected anomalies.\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess new data\n",
        "    # Extract date-based features if not already extracted\n",
        "    if 'day_of_week' not in data.columns:\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
        "        data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
        "        data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
        "        data['year'] = data['date'].dt.year             # Year\n",
        "        data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
        "\n",
        "    # Select relevant features\n",
        "    features = ['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']\n",
        "    data = data[features]\n",
        "\n",
        "    # Normalize the data using the scaler from training\n",
        "    data_scaled = scaler.transform(data)\n",
        "\n",
        "    # Step 2: Use model to reconstruct data\n",
        "    reconstructed = model.predict(data_scaled)\n",
        "    mse = np.mean(np.power(data_scaled - reconstructed, 2), axis=1)\n",
        "\n",
        "    # Step 3: Detect anomalies\n",
        "    # Set anomaly threshold\n",
        "    threshold = np.percentile(mse, percentile)\n",
        "\n",
        "    # Flag the anomalies\n",
        "    data['reconstruction_error'] = mse\n",
        "    data['is_anomaly'] = mse > threshold\n",
        "\n",
        "    # Step 4: Filter anomalies\n",
        "    anomalies = data[data['is_anomaly']]\n",
        "\n",
        "    # Step 5: Plot results\n",
        "    plot_results(data, anomalies)\n",
        "\n",
        "    return data, anomalies\n",
        "\n",
        "def training_pipeline(model, data, scaler, epochs=50, batch_size=16):\n",
        "    \"\"\"\n",
        "    Simplified pipeline to preprocess data and retrain an existing autoencoder model.\n",
        "\n",
        "    Parameters:\n",
        "        model (tf.keras.Model): Pre-trained autoencoder model.\n",
        "        data (pd.DataFrame): Input data with 'date' and 'amount' columns.\n",
        "        scaler (MinMaxScaler): Scaler used to preprocess the data during initial training.\n",
        "        epochs (int): Number of epochs for retraining.\n",
        "        batch_size (int): Batch size for training.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Retrained autoencoder model.\n",
        "    \"\"\"\n",
        "    # Step 1: Preprocess the data using the existing scaler\n",
        "    if 'day_of_week' not in data.columns:\n",
        "        data['date'] = pd.to_datetime(data['date'])\n",
        "        data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
        "        data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
        "        data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
        "        data['year'] = data['date'].dt.year             # Year\n",
        "        data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
        "        data = data[['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']]\n",
        "\n",
        "    data_scaled = scaler.transform(data)\n",
        "\n",
        "    # Step 2: Retrain the autoencoder\n",
        "    model.fit(data_scaled, data_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
        "\n",
        "    return model\n",
        "\n",
        "def generate_json(processed_data, anomalies, total_income, total_expense, advice_list):\n",
        "    \"\"\"\n",
        "    Generates a JSON report from the processed data, detected anomalies, and financial insights.\n",
        "\n",
        "    Parameters:\n",
        "        processed_data (pd.DataFrame): Dataframe with processed financial data.\n",
        "        anomalies (pd.DataFrame): Dataframe containing detected anomalies.\n",
        "        total_income (float): Total income in the dataset.\n",
        "        total_expense (float): Total expense in the dataset.\n",
        "        advice_list (list): List of financial advice as strings.\n",
        "\n",
        "    Returns:\n",
        "        str: JSON-formatted report as a string.\n",
        "    \"\"\"\n",
        "\n",
        "    net_cashflow = total_income - total_expense\n",
        "\n",
        "    anomaly_list = anomalies[['description', 'date', 'amount']].to_dict(orient='records')\n",
        "\n",
        "    report = {\n",
        "        \"cashflow_analysis\": {\n",
        "            \"total_income\": total_income,\n",
        "            \"total_expense\": total_expense,\n",
        "            \"net_cashflow\": net_cashflow\n",
        "        },\n",
        "        \"financial_advice\": advice_list,\n",
        "        \"anomaly_detection\": anomaly_list\n",
        "    }\n",
        "\n",
        "    return json.dumps(report, indent=2)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example dataset\n",
        "    data = pd.read_json('pengeluaran.json')\n",
        "\n",
        "    # Run the pipeline\n",
        "    autoencoder, processed_data, anomalies, scaler = main_pipeline(data, epochs=50, batch_size=4, percentile=99)\n",
        "\n",
        "    # Display results\n",
        "    print(\"Processed Data with Anomaly Status:\")\n",
        "    print(processed_data)\n",
        "\n",
        "    print(\"\\nDetected Anomalies:\")\n",
        "    print(anomalies)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRCYFYv_2Mv5"
      },
      "source": [
        "## Checking New Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYGDbvy12oxz"
      },
      "outputs": [],
      "source": [
        "bri_data = pd.read_json('bri_pengeluaran.json')\n",
        "\n",
        "predict_pipeline(model=autoencoder, data=bri_data, scaler=scaler, percentile=99)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmwHnnSdxQHk"
      },
      "outputs": [],
      "source": [
        "pf_data = pd.read_json('pf_pengeluaran.json')\n",
        "\n",
        "predict_pipeline(model=autoencoder, data=pf_data, scaler=scaler, percentile=99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxf810j-2Mv7"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_rVE_9qCchT"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan model dalam format HDF5\n",
        "# Menyimpan model ke file\n",
        "autoencoder.save('autoencoder_model.h5')  # Format HDF5\n",
        "# atau menggunakan format SavedModel\n",
        "autoencoder.export('autoencoder_model/')\n",
        "\n",
        "\n",
        "# Simpan scaler menggunakan pickle\n",
        "scaler_pickle_path = \"scaler.pkl\"\n",
        "with open(scaler_pickle_path, 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print(f\"Scaler saved to: {scaler_pickle_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQjDxFXxQDtn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}