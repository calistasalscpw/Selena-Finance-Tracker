{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3l1WBviGjzP5",
    "outputId": "6b216d4a-0ac9-4b30-af9b-9dc2956beea2"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as 'Mapping' could not be imported from 'c:\\Program Files\\Python312\\Lib\\collections\\__init__.py'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, losses\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data by extracting date-based features and normalizing values using MinMaxScaler.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input dataframe with 'date' and 'amount' columns.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Scaled feature matrix.\n",
    "        pd.DataFrame: Original data with extracted features.\n",
    "        MinMaxScaler: Fitted scaler for potential reuse.\n",
    "    \"\"\"\n",
    "    # Convert 'date' to datetime and extract features\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "    data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
    "    data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
    "    data['year'] = data['date'].dt.year             # Year\n",
    "    data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
    "\n",
    "    # Select relevant features\n",
    "    features = ['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']\n",
    "    data = data[features]\n",
    "\n",
    "    # Normalize the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "    return data_scaled, data, scaler\n",
    "\n",
    "\n",
    "# Function to build the autoencoder model\n",
    "def build_autoencoder(input_dim):\n",
    "    \"\"\"\n",
    "    Builds and compiles an autoencoder model.\n",
    "\n",
    "    Parameters:\n",
    "        input_dim (int): Number of input features.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled autoencoder model.\n",
    "    \"\"\"\n",
    "    autoencoder = models.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),       # Input layer\n",
    "        layers.Dense(32, activation='relu'),    # Encoder\n",
    "        layers.Dense(16, activation='relu'),    # Encoder\n",
    "        layers.Dense(8, activation='relu'),     # Bottleneck\n",
    "        layers.Dense(16, activation='relu'),    # Decoder\n",
    "        layers.Dense(32, activation='relu'),    # Decoder\n",
    "        layers.Dense(input_dim, activation='sigmoid')  # Output layer\n",
    "    ])\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(), loss=losses.MeanSquaredError())\n",
    "    return autoencoder\n",
    "\n",
    "# Function to detect anomalies\n",
    "def detect_anomalies(autoencoder, data_scaled, original_data, scaler, percentile=99):\n",
    "    \"\"\"\n",
    "    Detects anomalies using the trained autoencoder.\n",
    "\n",
    "    Parameters:\n",
    "        autoencoder (tf.keras.Model): Trained autoencoder model.\n",
    "        data_scaled (np.ndarray): Normalized input data.\n",
    "        original_data (pd.DataFrame): Original data with features.\n",
    "        scaler (MinMaxScaler): Scaler used for normalization.\n",
    "        percentile (float): Percentile threshold for anomaly detection.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data with anomaly status and reconstruction errors.\n",
    "        pd.DataFrame: Detected anomalies.\n",
    "    \"\"\"\n",
    "    # Reconstruct the input data\n",
    "    reconstructed = autoencoder.predict(data_scaled)\n",
    "    mse = np.mean(np.power(data_scaled - reconstructed, 2), axis=1)  # Reconstruction error\n",
    "\n",
    "    # Set anomaly threshold\n",
    "    threshold = np.percentile(mse, percentile)\n",
    "\n",
    "    # Flag anomalies\n",
    "    original_data['reconstruction_error'] = mse\n",
    "    original_data['is_anomaly'] = mse > threshold\n",
    "\n",
    "    # Filter anomalies\n",
    "    anomalies = original_data[original_data['is_anomaly']]\n",
    "\n",
    "    return original_data, anomalies\n",
    "\n",
    "# Function to plot the data\n",
    "def plot_results(processed_data, anomalies):\n",
    "    \"\"\"\n",
    "    Plots the data, highlighting anomalies in red and normal data in blue, with 'date' on the x-axis and 'amount' on the y-axis.\n",
    "\n",
    "    Parameters:\n",
    "        processed_data (pd.DataFrame): Dataframe with anomaly status.\n",
    "        anomalies (pd.DataFrame): Detected anomalies.\n",
    "    \"\"\"\n",
    "    # Reconstruct a synthetic 'date' from year and day_of_year\n",
    "    processed_data['date'] = pd.to_datetime(processed_data['year'] * 1000 + processed_data['day_of_year'], format='%Y%j')\n",
    "    anomalies['date'] = pd.to_datetime(anomalies['year'] * 1000 + anomalies['day_of_year'], format='%Y%j')\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot normal data\n",
    "    normal_data = processed_data[~processed_data['is_anomaly']]\n",
    "    plt.scatter(normal_data['date'], normal_data['amount'], color='blue', label='Normal', alpha=0.6)\n",
    "\n",
    "    # Plot anomalies\n",
    "    plt.scatter(anomalies['date'], anomalies['amount'], color='red', label='Anomaly', alpha=0.8)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.title('Anomaly Detection in Financial Transactions')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Amount')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Main function to run the pipeline\n",
    "def main_pipeline(data, epochs=50, batch_size=16, percentile=99):\n",
    "    \"\"\"\n",
    "    Main pipeline for anomaly detection using autoencoder.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): Input dataframe with 'date' and 'amount' columns.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size for training.\n",
    "        percentile (float): Percentile threshold for anomaly detection.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Trained autoencoder model.\n",
    "        pd.DataFrame: Processed data with anomaly status.\n",
    "        pd.DataFrame: Detected anomalies.\n",
    "        MinMaxScaler: Scaler used for data normalization.\n",
    "        float: Threshold for anomaly detection based on reconstruction error.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the data\n",
    "    data_scaled, processed_data, scaler = preprocess_data(data)\n",
    "\n",
    "    # Step 2: Build the autoencoder\n",
    "    input_dim = data_scaled.shape[1]\n",
    "    autoencoder = build_autoencoder(input_dim)\n",
    "\n",
    "    # Step 3: Train the autoencoder\n",
    "    history = autoencoder.fit(data_scaled, data_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
    "\n",
    "    # Step 4: Detect anomalies\n",
    "    processed_data, anomalies = detect_anomalies(autoencoder, data_scaled, processed_data, scaler, percentile=percentile)\n",
    "\n",
    "    # Step 5: Financial metrics for cashflow analysis\n",
    "    total_income = data[data['amount'] > 0]['amount'].sum()\n",
    "    total_expense = data[data['amount'] > 0]['amount'].sum()\n",
    "    advice_list = [\n",
    "        \"Reduce unnecessary spending\",\n",
    "        \"Plan your spendings wisely\"\n",
    "    ]\n",
    "    anomalies['description'] = \"Unusual spending detected\"\n",
    "\n",
    "    # Step 6: Generate JSON report\n",
    "    json_report = generate_json(processed_data, anomalies, total_income, total_expense, advice_list)\n",
    "    with open(\"result_data.json\", \"w\") as json_file:\n",
    "        json_file.write(json_report)\n",
    "    \n",
    "    # Step 7: Plot results\n",
    "    plot_results(processed_data, anomalies)\n",
    "\n",
    "    return autoencoder, processed_data, anomalies, scaler\n",
    "\n",
    "def predict_pipeline(model, data, scaler, percentile):\n",
    "    \"\"\"\n",
    "    Pipeline to preprocess data, predict anomalies, and plot results using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "        model (tf.keras.Model): Trained autoencoder model.\n",
    "        data (pd.DataFrame): New data with the same structure as training data.\n",
    "        scaler (MinMaxScaler): Scaler used to preprocess the training data.\n",
    "        percentile (float): Percentile for anomaly detection based on reconstruction error.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: New data with anomaly status and reconstruction errors.\n",
    "        pd.DataFrame: Detected anomalies.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess new data\n",
    "    # Extract date-based features if not already extracted\n",
    "    if 'day_of_week' not in data.columns:\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "        data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
    "        data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
    "        data['year'] = data['date'].dt.year             # Year\n",
    "        data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
    "\n",
    "    # Select relevant features\n",
    "    features = ['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']\n",
    "    data = data[features]\n",
    "\n",
    "    # Normalize the data using the scaler from training\n",
    "    data_scaled = scaler.transform(data)\n",
    "\n",
    "    # Step 2: Use model to reconstruct data\n",
    "    reconstructed = model.predict(data_scaled)\n",
    "    mse = np.mean(np.power(data_scaled - reconstructed, 2), axis=1)\n",
    "\n",
    "    # Step 3: Detect anomalies\n",
    "    # Set anomaly threshold\n",
    "    threshold = np.percentile(mse, percentile)\n",
    "\n",
    "    # Flag the anomalies\n",
    "    data['reconstruction_error'] = mse\n",
    "    data['is_anomaly'] = mse > threshold\n",
    "\n",
    "    # Step 4: Filter anomalies\n",
    "    anomalies = data[data['is_anomaly']]\n",
    "\n",
    "    # Step 5: Plot results\n",
    "    plot_results(data, anomalies)\n",
    "\n",
    "    return data, anomalies\n",
    "\n",
    "def training_pipeline(model, data, scaler, epochs=50, batch_size=16):\n",
    "    \"\"\"\n",
    "    Simplified pipeline to preprocess data and retrain an existing autoencoder model.\n",
    "\n",
    "    Parameters:\n",
    "        model (tf.keras.Model): Pre-trained autoencoder model.\n",
    "        data (pd.DataFrame): Input data with 'date' and 'amount' columns.\n",
    "        scaler (MinMaxScaler): Scaler used to preprocess the data during initial training.\n",
    "        epochs (int): Number of epochs for retraining.\n",
    "        batch_size (int): Batch size for training.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Retrained autoencoder model.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the data using the existing scaler\n",
    "    if 'day_of_week' not in data.columns:\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek  # Day of the week (0=Monday, 6=Sunday)\n",
    "        data['month'] = data['date'].dt.month           # Month of the year (1=January, 12=December)\n",
    "        data['day_of_month'] = data['date'].dt.day      # Day of the month (1-31)\n",
    "        data['year'] = data['date'].dt.year             # Year\n",
    "        data['day_of_year'] = data['date'].dt.dayofyear # Day of the year (1-365 or 366)\n",
    "        data = data[['amount', 'day_of_week', 'month', 'day_of_month', 'year', 'day_of_year']]\n",
    "\n",
    "    data_scaled = scaler.transform(data)\n",
    "\n",
    "    # Step 2: Retrain the autoencoder\n",
    "    model.fit(data_scaled, data_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_json(processed_data, anomalies, total_income, total_expense, advice_list):\n",
    "    \"\"\"\n",
    "    Generates a JSON report from the processed data, detected anomalies, and financial insights.\n",
    "\n",
    "    Parameters:\n",
    "        processed_data (pd.DataFrame): Dataframe with processed financial data.\n",
    "        anomalies (pd.DataFrame): Dataframe containing detected anomalies.\n",
    "        total_income (float): Total income in the dataset.\n",
    "        total_expense (float): Total expense in the dataset.\n",
    "        advice_list (list): List of financial advice as strings.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON-formatted report as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    net_cashflow = total_income - total_expense\n",
    "\n",
    "    anomaly_list = anomalies[['description', 'date', 'amount']].to_dict(orient='records')\n",
    "\n",
    "    report = {\n",
    "        \"cashflow_analysis\": {\n",
    "            \"total_income\": total_income,\n",
    "            \"total_expense\": total_expense,\n",
    "            \"net_cashflow\": net_cashflow\n",
    "        },\n",
    "        \"financial_advice\": advice_list,\n",
    "        \"anomaly_detection\": anomaly_list\n",
    "    }\n",
    "\n",
    "    return json.dumps(report, indent=2)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    data = pd.read_json('pengeluaran.json')\n",
    "\n",
    "    # Run the pipeline\n",
    "    autoencoder, processed_data, anomalies, scaler = main_pipeline(data, epochs=50, batch_size=4, percentile=99)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Processed Data with Anomaly Status:\")\n",
    "    print(processed_data)\n",
    "\n",
    "    print(\"\\nDetected Anomalies:\")\n",
    "    print(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WYGDbvy12oxz",
    "outputId": "61daa813-bf77-48f4-ffe0-890cd43e7fe0"
   },
   "outputs": [],
   "source": [
    "bri_data = pd.read_json('bri_pengeluaran.json')\n",
    "\n",
    "predict_pipeline(model=autoencoder, data=bri_data, scaler=scaler, percentile=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WmwHnnSdxQHk",
    "outputId": "0f52e480-597b-49be-88b4-05399422ce33"
   },
   "outputs": [],
   "source": [
    "pf_data = pd.read_json('pf_pengeluaran.json')\n",
    "\n",
    "predict_pipeline(model=autoencoder, data=pf_data, scaler=scaler, percentile=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_rVE_9qCchT",
    "outputId": "3ec2f8ae-e465-49a6-ddab-60d3ab90cea2"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Simpan model dalam format HDF5\n",
    "# Menyimpan model ke file\n",
    "autoencoder.save('autoencoder_model.h5')  # Format HDF5\n",
    "# atau menggunakan format SavedModel\n",
    "autoencoder.export('autoencoder_model/')\n",
    "\n",
    "\n",
    "# Simpan scaler menggunakan pickle\n",
    "scaler_pickle_path = \"scaler.pkl\"\n",
    "with open(scaler_pickle_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"Scaler saved to: {scaler_pickle_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQjDxFXxQDtn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
